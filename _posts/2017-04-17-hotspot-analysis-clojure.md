---
title: Hotspot analysis tools for Clojure
---

# Hotspot analysis tools for Clojure

Few weeks ago I decided to go and do some hotspot analysis on our codebase. For Java stuff I'm pretty confident to go with a lightweight sonar setup (actually I've not been using it in a while so that confidence might need some updating…), but for the Clojure bits I'm in the dark, so I did some quick duckduckgoing and asked around. I tried a few things on my train ride home at the end of the week, just to forget 1 hour after. So here's some writeup: mostly to supplement my failing memory, but I'll be glad if you find it useful.

The names that came up in my search are:
- eastwood
- kibit
- code-maat
- cloverage

# The droid I'm looking for

So first thing first, some context on what I'm looking for:

- areas where unit test coverage is low
- areas of high complexity
- circular dependencies among classes/namespaces
- stuff that has not been touched in a while

The idea is: confidence in tests is low, tests themselves are badly written & in most parts of the code "clean design" is a taboo word so let's find out which are the places that need love more critically.

Low coverage can easily be an indicator that code has been tested for success cases but we've been sloppy on the failure scenarios, so the behaviour of the software in such conditions becomes unpredictable.

High complexity can easily mean that a component grew from a single responsibility to multiple but similar responsibilities, causing "corner cases" to be bolted on the original code path.

Circular dependencies are bad bad bad hope it goes without saying?

If something has not been touched in a while it can mean 2 things: it either works damn well (then I'd expect it to be pulled in a library if it's reusable enough) or everyone's scared of touching it.

These things of course tend to overlap on the same piece of code as cause(s) and effect(s). And they can mean other things as well or none at all, depending on the history, practices and culture of your organization. For the context I'm in at the moment, the above statements hold with a fair degree of confidence.

Another thing well worth mentioning is that I do believe this kind of analysis is something that has to be performed as a spot thing: when metrics such coverage, complexity and age become KPIs it tends to drive behaviour that I wouldn't consider positive (e.g. someone gating a release b/c 0.01% missing coverage and it was even the wrong report ¯\_(ツ)_/¯ )

# The playground

Sooo I can't try this stuff out on the codebase I want to b/c that code is only accessible from the office and I rarely have enough spare time to experiment there. So to give it a spin from my laptop I'll use a very small Clojure webapp that does CRUD on a single entity with some custom upsert logic. This will potentially skew the results but I'll get a vague idea of whether it's valuable to spent some time runnig the tools against the beast or not.

# Eastwood

[Eastwood](https://github.com/jonase/eastwood) is a linter. We have it running as part of the build process and tbh I've never seen anything interesting coming from there. Possibly because the team uses a certain popular IDE which includes some auto-format functionality (I won't name it because I can't cope with it and just hate it but yeah, it's the one that is Java-based, handles a lot of languages and requires a license to be used). Anyway I can't really see how a lint report would give me any insight on the hotspots mentioned above, unless there's a high concentration of reported issues in the same spot?

A thing to note is that the analyzer it's based on fails when the generated bytecode method is >64KB and because of some transformations/optimizations in some Clojure things a medium sized battery of `midje` tests wrapped in a single `facts` can easily blow it up.

# Kibit

[Kibit](https://github.com/jonase/kibit) is a static analyzer. It searches for forms that can be simplified and suggests a replacement. Same considerations as Eastwood, I don't see how its reports can be of use in my case, except a high concentration in the same place as a proxy for complexity.

I might be missing something as the playground I'm running it on is too small to report anything at all.

# Code-maat

I remember [code-maat](https://github.com/adamtornhill/code-maat) from the book "Your Code as a Crime Scene" by Adam Tornhill. This sounds pretty much like exactly what I want to get insight about code age and halflife (and logical coupling, I forgot about that!). It's also the last tool I tried on my ride home and had to leave it because I arrived. I will pick it up again and expand on the point but for now:

- the playground has a short git history, a single author and only 2 files so there's no point using code-maat against it.
- it reports in CSV, visualizing the results requires pulling some other tool on top of it. Again, didn't try b/c time ran out but will get to it again and expand.

# Cloverage

[Cloverage](https://github.com/cloverage/cloverage) is, surprise surprise, a code coverage reporting tool. Should fit the bill for spotting areas that lack coverage, but running it against the playground (that I'm fairly sure has very high coverage) reports an embarassing 24% on forms and 38% on lines. I double check the tests and compare the code they exercise with what's highlighted in the HTML report and figure out that yeah, I actually didn't test some of that as it's mostly routing glue and I ended up overlooking it. Bad me. But the stuff that instead I indeed tested is not maked in green but comment-white: it seems that Cloverage completely ignores code inside a `reify` form.

I like the tool. The HTML report even highlights in yellow the lines that have coverage but not full branch coverage. It would be great if branch coverage appeared in the summary as well, I find it to be a more insightful metric than line coverage. And line-branch gaps are interesting warning signs.

# Conclusions

TL;DR: Cloverage's good, would be nice to see numbers for branching too. Code-maat will almost certaintly give me the insight I want but it needs some work (or cash to throw at the commercial version).

That's it for a couple of hours of looking around. This post will be a live document and I'll eventually update it with new findings.
